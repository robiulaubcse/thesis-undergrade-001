# -*- coding: utf-8 -*-
"""thesis_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LbxxiTErJj4pZKpYmvNI6KYKRtRLBd1_
"""

import cv2
import numpy as np
import pandas as pd
from google.colab.patches import cv2_imshow
import os
import cv2

# Mount Google Drive if using Google Colab
from google.colab import drive
drive.mount('/content/drive')

# Define the paths to your image folders
motion_blur_path = "/content/drive/MyDrive/Google Colab/Machine_Learning/data/thesis/images/motion_blurred"
defocused_blur_path = "/content/drive/MyDrive/Google Colab/Machine_Learning/data/thesis/images/defocused_blurred"
sharp_images_path = "/content/drive/MyDrive/Google Colab/Machine_Learning/data/thesis/images/sharp"

# Function to load and convert images to grayscale arrays
def load_images_and_labels(folder_path, label):
    images = []
    labels = []
    for filename in os.listdir(folder_path):
      img = cv2.imread(os.path.join(folder_path, filename), cv2.IMREAD_GRAYSCALE)
      images.append(img)
      labels.append(label)



    return images, labels

# Load images from each folder and assign labels
motion_blur_images, motion_blur_labels = load_images_and_labels(motion_blur_path, label=0)
defocused_blur_images, defocused_blur_labels = load_images_and_labels(defocused_blur_path, label=1)
sharp_images, sharp_labels = load_images_and_labels(sharp_images_path, label=2)

# Concatenate all images and labels
all_images =  motion_blur_images + defocused_blur_images + sharp_images
all_labels =  motion_blur_labels + defocused_blur_labels + sharp_labels

# Define function to perform gradient calculation and histogram generation

def process_image(image):
    # Calculate gradients using Sobel operators
    x_gradient_array = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)
    y_gradient_array = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)

    # Calculate magnitude of gradients
    magnitude_of_x_y = np.sqrt(x_gradient_array ** 2 + y_gradient_array ** 2)

    max_mag=np.max(magnitude_of_x_y)
    min_mag=np.min(magnitude_of_x_y)

    # Calculate histogram of magnitude
    hist_mag, _ = np.histogram(magnitude_of_x_y.ravel(), bins=np.arange(0, 951, 50))

    hist_mag_df = pd.DataFrame(hist_mag.reshape(1, -1))

    # Calculate histogram of directions
    direction = np.arctan2(y_gradient_array, x_gradient_array)
    max_dir=np.max(direction)

    min_dir=np.min(direction)

    hist_dir, _ = np.histogram(direction.ravel(), bins=np.arange(-4, 5))
    hist_dir_df = pd.DataFrame(hist_dir.reshape(1, -1))

    return hist_mag_df, hist_dir_df,max_mag,min_mag,max_dir,min_dir

#Execution time 7 minutes +
# Process all images
hist_mag_list = []
hist_dir_list = []
for img in all_images:
    hist_mag_df, hist_dir_df, max_mag, min_mag, max_dir, min_dir = process_image(img)
    tem_max_mag=max_mag
    if max_mag > tem_max_mag:
      tem_max_mag=max_mag

    tem_min_mag=min_mag
    if min_mag < tem_min_mag:
      tem_min_mag=min_mag

    tem_max_dir=max_dir
    if max_dir > tem_max_dir:
      tem_max_dir=max_dir


    tem_min_dir=min_dir
    if min_dir < tem_min_dir:
      tem_min_dir=min_dir


    hist_mag_list.append(hist_mag_df)
    hist_dir_list.append(hist_dir_df)

# Concatenate dataframes
final_hist_mag = pd.concat(hist_mag_list, ignore_index=True)
final_hist_dir = pd.concat(hist_dir_list, ignore_index=True)

print('Magnitude_Max_Min:',tem_max_mag, tem_min_mag)
print('Direction_Max_Min:',tem_max_dir, tem_min_dir)

# Define the bin ranges for magnitude and direction histograms
mag_bins = np.arange(0, 951, 50)
dir_bins = np.arange(-4, 5)

# Function to generate column names based on bin ranges
def generate_column_names(bins):
    column_names = []
    for i in range(len(bins) - 1):
        column_names.append(f'{bins[i]}-{bins[i+1]}')
    return column_names

# Generate column names for magnitude and direction histograms
mag_column_names = generate_column_names(mag_bins)
dir_column_names = generate_column_names(dir_bins)

# Assign column names to the DataFrame
final_hist_mag.columns = mag_column_names
final_hist_dir.columns = dir_column_names

"""Extra Code"""

import cv2
import numpy as np
import matplotlib.pyplot as plt

# Function to calculate magnitude and direction histograms
def calculate_histograms(image):
    # Calculate gradients using Sobel operators
    x_gradient_array = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)
    y_gradient_array = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)

    # Calculate magnitude of gradients
    magnitude_of_x_y = np.sqrt(x_gradient_array ** 2 + y_gradient_array ** 2)

    # Calculate direction of gradients
    direction = np.arctan2(y_gradient_array, x_gradient_array)

    # Calculate histograms
    max_mag = np.max(magnitude_of_x_y)
    mag_hist, mag_bins = np.histogram(magnitude_of_x_y.ravel(), bins=int(max_mag), range=(0, max_mag))
    dir_hist, dir_bins = np.histogram(direction.ravel(), bins=4, range=(-np.pi, np.pi))

    return mag_hist, mag_bins, dir_hist, dir_bins

# Load image
image = cv2.imread('/content/drive/MyDrive/Google Colab/Machine_Learning/data/thesis/images/3_HUAWEI-NOVA-LITE_F.jpg', cv2.IMREAD_GRAYSCALE)

# Calculate histograms
mag_hist, mag_bins, dir_hist, dir_bins = calculate_histograms(image)

# Plot magnitude histogram
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.plot(mag_bins[:-1], mag_hist, color='b')
plt.title('Magnitude Histogram')
plt.xlabel('Magnitude')
plt.ylabel('Frequency')

# Plot direction histogram
plt.subplot(1, 2, 2)
plt.plot(dir_bins[:-1], dir_hist, color='r')
plt.title('Direction Histogram')
plt.xlabel('Direction (radians)')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

# Convert labels list to a DataFrame
labels_df = pd.DataFrame({'label': all_labels})

# Concatenate final_hist_mag, final_hist_dir, and labels_df along columns
merged_df = pd.concat([final_hist_mag, final_hist_dir], axis=1)

print(labels_df.size)

merged_df.head()

#new first_df use in the second part
new_first_df = merged_df.iloc[:-350]

from sklearn.preprocessing import StandardScaler


# Initialize StandardScaler
scaler = StandardScaler()

# Fit and transform the data
scaled_data = scaler.fit_transform(merged_df)

print("\nScaled data using Standardization:")
print(scaled_data)

from sklearn.model_selection import train_test_split

X = scaled_data
y = labels_df

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,shuffle=True)

#from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

X = scaled_data
y = labels_df

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Create a Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the classifier on the training data
rf_classifier.fit(X_train, y_train)

# Make predictions on the testing data
y_pred = rf_classifier.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print("Accuracy:", accuracy)

# Display classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))



"""#second part"""





import cv2
import numpy as np
from skimage import restoration
import pandas as pd
import os
from multiprocessing import Pool



def spatial_features(image):
    # Convert image to grayscale
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Calculate spatial features
    mean_intensity = np.mean(gray_image)
    std_dev_intensity = np.std(gray_image)

    # Calculate edge features using Canny edge detector
    edges = cv2.Canny(gray_image, 50, 150)
    edge_density = np.sum(edges) / (gray_image.shape[0] * gray_image.shape[1])

    return [mean_intensity, std_dev_intensity, edge_density]

def frequency_features(image):
    # Convert image to grayscale
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Calculate Fourier Transform
    f_transform = np.fft.fft2(gray_image)
    f_transform_shifted = np.fft.fftshift(f_transform)
    magnitude_spectrum = 20 * np.log(np.abs(f_transform_shifted))

    # Calculate frequency features
    mean_spectrum = np.mean(magnitude_spectrum)
    std_dev_spectrum = np.std(magnitude_spectrum)

    # PSF recovery
    psf_size = 5
    psf = np.ones((psf_size, psf_size)) / psf_size**2
    blurred_image = restoration.wiener(gray_image, psf, 0.01)
    initial_psf = np.random.rand(psf_size, psf_size)
    recovered_psf, _ = restoration.unsupervised_wiener(blurred_image, initial_psf)

    # Calculate FWHM from PSF matrix
    fwhm = calculate_fwhm(recovered_psf)

    return [mean_spectrum, std_dev_spectrum, fwhm]



def calculate_fwhm(psf_matrix):
    # Find the maximum intensity value in the PSF matrix
    max_intensity = np.max(psf_matrix)

    # Find the indices where the intensity is greater than half of the maximum intensity
    above_half_max = psf_matrix > 0.5 * max_intensity

    # Find the first and last indices where the intensity is above half of the maximum
    first_index = np.argmax(above_half_max)
    last_index = len(above_half_max) - 1 - np.argmax(above_half_max[::-1])

    # Calculate the FWHM
    fwhm = abs(last_index - first_index)

    return fwhm

def extract_features(image):
    spatial_feats = spatial_features(image)
    frequency_feats = frequency_features(image)
    return spatial_feats + frequency_feats

def load_images_and_labels(image_folder, label, resize_shape):
    images = []
    labels = []
    for filename in os.listdir(image_folder):
        image_path = os.path.join(image_folder, filename)
        image = cv2.imread(image_path)
        resized_image = cv2.resize(image, resize_shape)
        images.append(resized_image)
        labels.append(label)
    return images, labels

# Define image resize shape
resize_shape = (255, 255)

# Load images and labels from the motion blurred folder
motion_blurred_folder = "/content/drive/MyDrive/Google Colab/Machine_Learning/data/thesis/images/motion_blurred"
motion_blurred_images, motion_blurred_labels = load_images_and_labels(motion_blurred_folder, label=0, resize_shape=resize_shape)

# Load images and labels from the defocused blurred folder
defocused_blurred_folder = "/content/drive/MyDrive/Google Colab/Machine_Learning/data/thesis/images/defocused_blurred"
defocused_blurred_images, defocused_blurred_labels = load_images_and_labels(defocused_blurred_folder, label=1, resize_shape=resize_shape)


# Load images and labels from the sharp folder
#sharp_images_folder = "/content/drive/MyDrive/Google Colab/Machine_Learning/data/thesis/images/sharp"
#sharp_images, sharp_labels = load_images_and_labels(sharp_images_folder, label=2, resize_shape=resize_shape)



# Concatenate data
images = motion_blurred_images + defocused_blurred_images
labels = motion_blurred_labels + defocused_blurred_labels

# Extract features using multiprocessing
with Pool() as pool:
    X = pool.map(extract_features, images)

# Create DataFrame
feature_names = ["mean_intensity", "std_dev_intensity", "edge_density",
                 "mean_spectrum", "std_dev_spectrum", "fwhm"]
df = pd.DataFrame(X, columns=feature_names)
#df['Label'] = labels

# Save DataFrame to CSV file
df.to_csv("image_features.csv", index=False)

df.head()

merged_df_full = pd.concat([new_first_df, df], axis=1)

merged_df_full.head()



from sklearn.preprocessing import StandardScaler


# Initialize StandardScaler
scaler = StandardScaler()

# Fit and transform the data
scaled_data_full = scaler.fit_transform(merged_df_full)

print("\nScaled data using Standardization:")
print(scaled_data_full)



full_df = pd.DataFrame(scaled_data_full, columns=['0 to 50', '50 to 100', '100 to 150','150 to 200','200 to 250','250 to 300','300 to 350','350 to 400','400 to 450','450 to 500','500 to 550','550 to 600','600 to 650','650 to 700','700 to 750','750 to 800','800 to 850','850 to 900','900 to 950','-4 to -3','-3 to -2','-2 to -1','-1 to 0','0 to 1','1 to 2','2 to 3','3 to 4','mean_intensity','std_dev_intensity','edge_density','mean_spectrum','std_dev_spectrum','fwhm'])

full_df.head()



from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Split the data into features (X) and labels (y)
X = full_df
y = labels

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train SVM classifier
svm_classifier = SVC(kernel='linear')
svm_classifier.fit(X_train, y_train)

# Train Random Forest classifier
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(X_train, y_train)

# Predict labels for testing data
svm_pred = svm_classifier.predict(X_test)
rf_pred = rf_classifier.predict(X_test)

# Evaluate performance
svm_accuracy = accuracy_score(y_test, svm_pred)
rf_accuracy = accuracy_score(y_test, rf_pred)

print("SVM Accuracy:", svm_accuracy)
print("Random Forest Accuracy:", rf_accuracy)

# Print classification report for SVM
print("SVM Classification Report:")
print(classification_report(y_test, svm_pred))

# Print classification report for Random Forest
print("Random Forest Classification Report:")
print(classification_report(y_test, rf_pred))

